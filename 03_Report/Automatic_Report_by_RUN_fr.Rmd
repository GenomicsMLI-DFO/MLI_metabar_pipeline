---
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = "png",
                      dpi = 150)
knitr::opts_knit$set(root.dir = here::here())

`%nin%` = Negate(`%in%`)

# Projet à analyser

projet <- c("TE") 
sequencer <- "NovaSeq 6000 SP PE250"

# RUN à analyser

RUN <- "MI_3992"

# Données initiales
source(file.path(here::here(), "01_Code", "Functions", "get.value.R"))
library(tidyverse)
library(ggforce)

LOCUS <- stringr::str_split(get.value("Loci", file = file.path(here::here(), "Options.txt")), pattern = ";")[[1]]

SUBGROUP <- stringr::str_split(get.value("group.metabaR", file = file.path(here::here(), "Options.txt")), pattern = ";")[[1]]

data.info <- readr::read_csv(file.path(here::here(), "00_Data", "00_FileInfos", "SeqInfo.csv") )

data.info <- data.info %>% mutate(Type_echantillon = factor(Type_echantillon, levels = c("ECH", "FNC", "SNC", "FNC_SNC", "SNC_FNC", "ENC", "PNC", "MNC", "PPC", "MPC")))

# Create a list of which PCR to be considered in each SUBGROUP

SUBGROUP.ls <- list()

for(x in SUBGROUP){
  
  subgroup <- c(data.info %>% dplyr::filter(ID_subprojet == x)  %>% dplyr::filter(Run == RUN) %>% pull(ID_subprojet) %>% unique(),
                data.info %>% dplyr::filter(ID_subprojet == x)  %>% dplyr::filter(Run == RUN) %>% pull(ID_projet) %>% unique(),                                                         
                "ALL", "All", "all") %>% unique()
  
  id <- data.info %>% dplyr::filter(ID_subprojet %in% subgroup) %>% dplyr::filter(Run == RUN)  %>% pull(ID_labo) %>% unique()
  
  SUBGROUP.ls[[x]] <- id
  
  
}


```

<!-- Page titre -->
\clearpage
\thispagestyle{empty}
\centering
&nbsp;
\vfill
\huge Rapport d'analyses post-séquençage de librairies de métacodage à barres

\vfill
\Large Projet `r projet`

\vfill
\normalsize
Produit par le Laboratoire de génomique

Institut Maurice-Lamontagne (IML) 

Pêches et Océans Canada

Mont-Joli, Canada

\vfill
Préparation des librairies : Piere-Jean Jacques, pierre-jean.jacques@dfo-mpo.gc.ca

Analyses bioinformatiques :  Jane Doe, jane.doe@dfo-mpo.gc.ca

Révision : John Doe, john.doe@dfo-mpo.gc.ca

\vfill
`r Sys.Date()`

\raggedright


\newpage

\setcounter{tocdepth}{2}
\renewcommand*\contentsname{Table des matières}
\tableofcontents 

\newpage

# 1. Objectif général du rapport

Ce rapport automatisé permet un survol qualitatif des données générées par le pipeline d’analyse en métacodage à barres  **[MLI_metabar_pipeline](https://github.com/GenomicsMLI-DFO/MLI_metabar_pipeline) (v.`r    get.value("MLI.version")`)**. Il permet aussi d’avoir une description générales du jeu de données qui a été analysé, des étapes et des paramètres d'analyses dans le pipeline et des résultats préliminaires de détections d'ADNe pour guider les prochaines étapes du projet.

Les résultats présentés ne peuvent être considérés comme finaux et ne peuvent être publiés. Ils devraient toujours être interprétés avec précaution.

\newpage

# 2. Description générale du jeux de données

```{r echo=FALSE}
n.samples.tot <- data.info %>% dplyr::filter(Run == RUN) %>% nrow()
n.loc.tot <- data.info %>% dplyr::filter(Run == RUN) %>% pull(Loci) %>% unique() %>% length() 

n.samples.projet <- data.info %>% filter(ID_labo %in% SUBGROUP.ls[[projet]], Loci %in% LOCUS) %>% pull(ID_labo) %>% length()
n.samples.ech <- data.info %>% filter(ID_labo %in% SUBGROUP.ls[[projet]], Type_echantillon %in% c("ECH"), Loci %in% LOCUS) %>% pull(ID_labo) %>% length()
n.samples.control <- n.samples.projet - n.samples.ech 

```

Les données ont été séquencées sur une run de *`r sequencer`* (`r RUN`) comportant au total **`r n.samples.tot`** échantillons (pcr) et `r n.loc.tot` locus. Le projet **`r projet`** comprend un total de **`r n.samples.projet`** échantillons (pcr; incluant `r n.samples.control` contrôles), et **`r length(LOCUS)`** locus (**`r paste(LOCUS, collapse = ", ")`**). 

```{r message=FALSE, warning=FALSE, include=FALSE}
if(length(SUBGROUP) == 1){
   
  project.text <- paste("Les échantillons de ce projet ont été analysés seuls (sans autres projets)")  
  
}

if(length(SUBGROUP) == 2){
  
   project.text <- paste("Les échantillons de ce projet ont été analysés avec 1 autre projet (",  paste(SUBGROUP %>% str_subset(projet, negate = T), collapse = ", "), ").") 
  
}  

if(length(SUBGROUP) > 1){
  
   project.text <- paste("Les échantillons de ce projet ont été analysés avec", length(SUBGROUP) - 1,
                         "autres projets (",  paste(SUBGROUP %>% str_subset(projet, negate = T), collapse = ", "), ").") 
  
}  
  
```

`r project.text`


```{r echo=FALSE, message=FALSE, warning=FALSE}

summary.1 <- data.info %>% filter(ID_labo %in% SUBGROUP.ls[[projet]],
                                   Run == RUN,
                     Loci %in% LOCUS) %>% 
              group_by(Loci, Type_echantillon) %>% 
              summarise(N = n()) %>% 
               pivot_wider(names_from = Type_echantillon, values_from = N)


summary.1  %>% ungroup() %>% mutate(Total = select(. , -1) %>% rowSums()) %>% 
              knitr::kable(caption = "Nombre d'échantillons (pcrs) par locus et catégorie")


```

**Légende**

ECH = Échantillon

SNC = Contrôle négatif de terrain (*sampling negative control*)

FNC = Contrôle négatif de filtration (*filtration negative control*)

ENC = Contrôle négatif d'extraction (*extraction negative control*)

PNC/NTC/MNC = Contrôle negatif de PCR (*PCR negative control*)

PPC = Contrôle positif de PCR (*PCR positive control*)

\newpage

# 3. Sommaire du pipeline d'analyse

```{r nreads, echo=FALSE, message=FALSE, warning=FALSE}
# Load cutadapt summary

cutadapt.res <- read_csv(file.path(here::here(), "00_Data", "02a_Cutadapt", "log", "Cutadapt_Stats.csv"))

esv.reads.res <- read_csv(file.path(here::here(), "02_Results", "02_Filtrations", "ESVtab_Stats_Nreads.csv"))
names(esv.reads.res)[4] <- "ESVdada2"

dada2.summary <- data.frame()

for(l in LOCUS){
  
  dada2.int <- readr::read_csv(file.path(here::here(), "00_Data", "02b_Filtered_dada2", "log", paste0(l, "_dada2_summary.csv"))) 
  dada2.int <- dada2.int %>% mutate(Loci = l)
  dada2.summary <- bind_rows(dada2.summary, dada2.int)
  
}
         
dada2.summary <- dada2.summary %>% mutate(Trim = reads.out,
                                          Adapt = reads.in
                                          ) %>% 
  select(-c(reads.in, reads.out))

# Load post tag res

tag.summary <- data.frame()

for(l in LOCUS){

  tag.int <- readr::read_csv(file.path(here::here(), "00_Data", "04_ESVcorrected", paste0("ESVtab_Stats_postTagjump_",RUN, "_", l, ".csv")),
                             col_types = "cddddc")

  tag.summary <- bind_rows(tag.summary, tag.int)
  
}

metabar.summary <- data.frame()

for(l in LOCUS){
  metabar.int <- readr::read_csv(file.path(here::here(), "00_Data", "04_ESVcorrected", paste0("ESVtab_Stats_postMetabaR_", l, "_",projet, ".csv")),
                                       col_types = "cddddcc") 

  metabar.summary <- bind_rows(metabar.summary, metabar.int)
  
}

nreads.res <- cutadapt.res %>% left_join(dada2.summary %>% mutate( ID_labo = ID %>% str_remove(paste(paste0("_", LOCUS), collapse = "|"))) %>%
                                select(ID_labo, Loci, Trim)) %>% 
                               left_join(esv.reads.res) %>% 
                               left_join(tag.summary %>% select(ID_labo = sample_id, ESVtagjump = nb_reads.tagjump, Loci)) %>% 
   left_join(metabar.summary %>% select(ID_labo = sample_id, ESVfinal = nb_reads_postmetabaR, Loci)) %>% 
               pivot_longer(c(Raw, Adapt, Trim, Merge, ESVdada2, ESVtagjump, ESVfinal), names_to = "Step", values_to = "Nreads") %>%
  left_join(data.info %>% select(ID_labo, Loci, ID_projet, Type_echantillon, Run), 
                           by = c("ID_labo", "Loci"))  %>%                           
   mutate(Nreads = ifelse(is.na(Nreads), 0, Nreads),
                                    Nreads = ifelse((Type_echantillon != "ECH" & Step == "ESVfinal"), NA, Nreads ),
                                    Nreads1 = Nreads + 1 ,
                        Step = factor(Step, levels = c("Raw", "Adapt", "Trim", "Merge", "ESVdada2", "ESVtagjump", "ESVfinal"))) 

# git project

git.url <- system2(command = "git", args = "remote get-url origin", stdout = T)

```

Le pipeline d'analyse se divise en sept grandes étapes qui ont une influence à la fois sur le nombre de reads (séquences) et le nombre de MOTUs (*Molecular operational taxnomique unit*, des OTUs, des ASVs, des ESVs ou autres) observé par échantillon. Dans le cas de ce rapport, les MOTUs sont des ESVs. Les grandes étapes sont :

1. Les données brutes (*Raw*)
2. Le retrait des adapteurs (*Adapt*)
3. La filtration sur la qualité (*Trim*)
4. La fusion des reads R1 et R2 (*Merge*)
5. Le retrait des chimères et la création d'une première table de MOTUs (*ESVdada2*)
6. La correction pour le tag-jumping (*ESVtagjump*)
7. La correction pour les contaminants, et le retrait des échantillon contrôles (*ESVfinal*)

Les scripts et résultats se trouvent sur GitHub dans le répertoire **[`r git.url %>% str_remove("https://github.com/")`](`r git.url`)**. Une description plus détaillée du pipeline utilisé est disponible à l'[Annexe I](#annexe-i).

Le profil de variation en nombre de reads est différent selon l'étape du pipeline (généralement plus prononcé à étape 4), le locus et le type d'échantillon (contrôle ou non). 


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5,  fig.width=7}

# Load post Metaba

nreads.res %>%  filter(ID_labo %in% SUBGROUP.ls[[projet]],
                       Run == RUN,
                      Loci %in% LOCUS) %>% 
  mutate(Cat_echantillon = ifelse(Type_echantillon %in% c("ECH"), "Échantillon", "Contrôle")
        ) %>% 
  ggplot(aes(x = Step, y = Nreads1, col = Type_echantillon, group = ID_labo)) +
  #geom_jitter(height = 0) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) + 
  #geom_boxplot() +
  scale_y_continuous(trans="log10") +
  facet_grid(Cat_echantillon ~ Loci) +
    labs(y = "N reads + 1 (log)", x = "Étape du pipeline", title = "Variation en N reads entre les étapes du pipeline")+ 
  theme_bw()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))


```

Le patron variation en nombre de MOTUs est aussi différent selon l'étape du pipeline, le locus et le type d'échantillon (contrôle ou non). À noter que les MOTUs sont obtenus seulement à l'étape 4 lors de la fusion des reads R1 et R2.


```{r nmotus, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=7}
# Stats at the MOTUS level
esv.motus.res <- read_csv(file.path(here::here(), "02_Results", "02_Filtrations", "ESVtab_Stats_NESV.csv"))
names(esv.motus.res)[4] <- "ESVdada2"

nmotus.res <- esv.motus.res %>% left_join(tag.summary %>% select(ID_labo = sample_id, ESVtagjump = nb_motus.tagjump, Loci)) %>% 
     left_join(metabar.summary %>% select(ID_labo = sample_id, ESVfinal = nb_motus_postmetabaR, Loci)) %>% 
               pivot_longer(c(Merge, ESVdada2, ESVtagjump, ESVfinal), names_to = "Step", values_to = "Nmotus") %>% 
                 left_join(data.info %>% select(ID_labo, Loci, ID_projet, Type_echantillon, Run), 
                           by = c("ID_labo", "Loci")) %>%
                 mutate(Nmotus = ifelse(is.na(Nmotus), 0, Nmotus),
                        Nmotus = ifelse((Type_echantillon != "ECH" & Step == "ESVfinal"), NA, Nmotus ),
                        Nmotus1 = Nmotus + 1 ,
                        Step = factor(Step, levels = c("Raw", "Adapt", "Trim", "Merge", "ESVdada2", "ESVtagjump", "ESVfinal"))) 


nmotus.res %>%  filter(ID_labo %in% SUBGROUP.ls[[projet]],
                       Run == RUN,
                      Loci %in% LOCUS) %>% 
    mutate(Cat_echantillon = ifelse(Type_echantillon %in% c("ECH"), "Échantillon", "Contrôle")
        ) %>% 
  ggplot(aes(x = Step, y = Nmotus1, col = Type_echantillon, group = ID_labo)) +
  #geom_jitter(height = 0) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) + 
  #geom_boxplot() +
 scale_y_continuous(trans="log10") +
  facet_grid(Cat_echantillon ~ Loci) +
    labs(y = "N motus + 1 (log)", x = "Étape du pipeline", title = "Variation en N MOTUs entre les étapes du pipeline")+ 
  theme_bw()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))



```

\newpage

# 4. Assignations taxonomiques

```{r echo=FALSE, message=FALSE, warning=FALSE}

assign.1 <- stringr::str_split(get.value("assign.1", file = file.path(here::here(), "Options.txt")), pattern = ";")[[1]]

assign.2 <- stringr::str_split(get.value("assign.2", file = file.path(here::here(), "Options.txt")), pattern = ";")[[1]]

if(assign.1[2] == "TOP"){
  assign.1.method <- "TopHit" 
} else {
    assign.1.method <- assign.1[2]
  
}

if(assign.2[2] == "TOP"){
  assign.2.method <- "TopHit" 
} else {
    assign.2.method <- assign.2[2]
  
}

# Message si tout a été fait avec blast NCBI
if((paste(assign.1, collapse = " ") == paste(assign.2, collapse = " ")) & (assign.1[1] == "Blast")){

msg1 <- paste("Les assignations taxonomiques utilisées dans les sections suivantes, autant pour les corrections des tables d'ESV que les assignations taxonomiques présentées, ont été effectuées à l'aide d'une version interne de la banque de séquences publiques **NCBI-nt** (alias GenBank) et l'outil d'assignation **blastn** combiné à une approche de",  paste0("**", assign.1.method), assign.1[3],"%**. Cette approche a été déterminée comme efficace au locus COI à partir d'un jeu de données du golfe Saint-Laurent. Toutefois, les résultats de ces analyses montrent aussi que les assignations à l'espèce sont de plus grande fiabilité lorsqu'une banque régionale de séquences références est utilisée.")  
  
}

# Message si tout a été fait avec un truc local
if((paste(assign.1, collapse = " ") == paste(assign.2, collapse = " ")) & (assign.1[1] == "Blast.local")){

msg1 <- paste("Les assignations taxonomiques utilisées dans les sections suivantes, autant pour les corrections des tables d'ESV que les assignations taxonomiques présentées, ont été effectuées à l'aide d'une **banque de séquences références locales** et l'outil d'assignation **blastn** combiné à une approche de", paste0("**",  assign.final.method), assign.final[3],"%**. Bien que les banques locales peuvent être plus fiables que celles publiques, certaines précautions devraient être prises afin de s'assurer que des espèces importantes ne sont pas passées sous le radar.")  

} 

# Message bonne combinaison publique et local 

if((assign.2[1] == "Blast") & (assign.1[1] == "Blast.local") ){

msg1 <- paste("Les assignations taxonomiques utilisées dans les sections suivantes, autant pour les corrections des tables d'ESV que les assignations taxonomiques présentées, sont issues de séquences références locales** et l'outil d'assignation **blastn** combiné à une approche de",  paste0("**", assign.1.method), assign.1[3],"%** complémenté d'une version interne de la banque de séquences publiques **NCBI-nt** (alias GenBank) et l'outil d'assignation **blastn** combiné à une approche de", paste0("**", assign.2.method), assign.2[3],"%** pour les MOTUs non-identifiés par la première approche.")

}


```

Les assignations taxonomiques, soit la comparaison des MOTUs observées avec des banques de séquences références, peuvent s'effectuer à l'aide de diverses méthodes qui fournissent des informations souvent complémentaires. Les banques de séquences publiques comme NCBI-nt (alias GenBank) sont utiles car elles comportent le plus de séquences, mais sont sensible aux erreurs d'identifications et peinent à identifier certains groupes dont la diversité génétique est faible. De l'autre côté, les banques de séquences locales comportent souvent moins d'erreurs et sont plus efficaces pour les groupes peu diversifiés, mais peuvent également manquer certaines espèces non-incluses comme les espèces invasives. L'utilisation de différentes banques de séquences références, particulièrement à la fois d'une banque locale/régionale et d'une banque publique, améliore la fiabilité des détections.

## 4.1 Assignations taxonomiques utilisées 

`r msg1`

```{r echo=FALSE, message=FALSE, warning=FALSE}

assign.final.path <- file.path(here::here(), "02_Results/03_TaxoAssign/Assignements.Final.csv")

if(file.exists(assign.final.path)){
  
  assign.final.res <- read_csv(assign.final.path)
}

if(paste(assign.2, collapse = " ") != paste(assign.1, collapse = " " )){

  assign.final.phylum <- assign.final.res %>%
      mutate(Approach = ifelse(Script == "Blast", "NCBI (Genbank)",
                                          ifelse(Script == "Blast.local", "Banque locale", Script))) %>% 
    group_by(Loci,phylum, Approach) %>% 
    summarise(N = n()) %>% 
    mutate(Total = sum(N),
           Prop = N/Total)
  
  for(l in LOCUS){
  
  gg.taxo <- assign.final.phylum %>%  dplyr::filter(Loci == l ) %>% 
    ggplot(aes(x = phylum, y = Prop, fill = Approach)) +
      geom_bar(stat = "identity", col = "black") +
      #facet_grid(Loci ~ ., scale = "free", space = "free")+
    scale_fill_manual(values = c("white", "gray"))+
      geom_text(aes(y = 0, label = paste0("n=",Total)), vjust = 0, hjust = 0, angle = 90, col = "black") +
    labs(y = "Prop. of assigned MOTUs",
         title = paste("Provenance des assignations pour", l)) +
      theme_bw() +
    theme(axis.text.x = element_text(angle = 90, h = 1, vjust = 0.5))
  print(gg.taxo)
  
}
  }


```

Si les résultats d'assignations devaient être utilisés pour publication, ces derniers devraient être révisés par un expert en taxonomie. Nous déconseillons fortement la publication de ces résultats à cette étape de l'automatisation du rapport.



## 4.2 Comparaison des taxons détectés

Outre les assignations taxonomiques présentées dans ce rapport, d'autres méthodes d'assignations ont été effectuées par le pipeline d'analyse et sont disponible au besoin.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

assign.path <- file.path(here::here(), "02_Results/03_TaxoAssign/Assignements.ALL.csv")

if(file.exists(assign.path)){
  
  assign.res <- read_csv(assign.path)
   assign.res  <- assign.res %>%
                 dplyr::mutate(Approach = ifelse(Script == "Blast", "NCBI (Genbank)",
                                          ifelse(Script == "Blast.local", "Banque locale", Script)),
                               Method = ifelse(Method %in% c("LCA", "TOP"), paste("Blast", Method), Method),
                               RefSeq = RefSeq %>% stringr::str_remove(".fasta"))
  assign.res %>%   dplyr::select(Loci,  Approach, RefSeq, Method, Threshold) %>% dplyr::distinct(.keep_all = T) %>% 
    dplyr::group_by(Loci,  Approach, Method) %>% summarise(RefSeq = paste(unique(RefSeq), collapse = ", "),
                                                             Threshold = paste(unique(Threshold), collapse = ", ")) %>% 
     knitr::kable(caption = "Information sur les differentes assignations taxonomiques produites par le pipeline.")
  
}


```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

# Fonction pour filtrer les lignes où toutes les méthodes sont "vrai"
filtrer_lignes <- function(data, num_colonnes_non_methodes) {
  # Utilisation de Reduce avec la fonction "&" pour vérifier si toutes les méthodes sont "vrai"
  condition <- Reduce(`&`, lapply((num_colonnes_non_methodes + 1):ncol(data), function(i) data[, i] == T))
  
  # Filtrage des lignes
  data_filtre <- data[!condition, ]
  
  return(data_filtre)
}

if(!is.null(assign.res)){
  # Run it only if there is more than 1 RefSeq
  if(length(unique(assign.res$RefSeq)) > 1){
  sp.test <-  assign.res %>% dplyr::filter(Levels %in% c("species"),
                               kingdom %in% c("Metazoa", "Animalia")) %>% 
  
    dplyr::group_by(Taxon, phylum, Loci, RefSeq) %>% 
    summarise(Detected = T) %>% 
    dplyr::ungroup() %>% 
    tidyr::pivot_wider(names_from =  RefSeq, values_from = Detected,  values_fill = F)  %>% 
    filtrer_lignes(3)
  
  gg.assign <- sp.test %>%  tidyr::gather( key = "RefSeq", value = "Detected", -c(Taxon, phylum, Loci)) %>% 
    ggplot(aes(y = Taxon, x = RefSeq, fill = Detected)) +
    geom_bin2d() +
    facet_grid(phylum ~ Loci, space = "free", scale = "free") +
    labs(title ="Comparaison des détections à l'espèce possibles",
         x = "Séquences références utilisées")
    theme_bw()

  print(gg.assign)  
  
  }
}

```

\newpage

# 5 Correction des tables de MOTUs

Différentes sources de contaminations ont été revues au cours du pipeline d'analyse. De plus, les échantillons problématiques ont été identifiés. Voici les grandes lignes des résultats. 

## 5.1 Correction pour le tag jumping

Le tag jumpping est une problématique importante pouvant créer un bruit de fond sur l'ensemble des échantillons et amener des problèmes de faux positifs (détections d'espèces non présentes). Pour compenser l'effet du tag jumping, un seuil minimal de N reads est déterminé par MOTUs. Le choix de ce seuil est important car il devrait réfléter un point d'inflexion où il y a une diminution dans la diversité des MOTUs et que le nombre de reads reste stable. Augmenter ou diminuer ce seuil a une incidence directe sur le taux de faux négatifs et faux positifs. 

Pour choisir le meilleur seuil possible, une attention particulière est donnée à l'impact dans les témoins négatifs, le but étant de minimiser le nombre de détections dans ces témoins, particulièrement dans les contrôles négatifs de PCR (PNC, MNC).


```{r echo=FALSE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE}
  metabar.param <- readr::read_tsv(file = file.path(here::here(), "01_Code/Parameters/metabar_param.tsv"))
metabar.param <- metabar.param %>% dplyr::filter(Locus %in% LOCUS)

tag.param <- metabar.param %>% dplyr::filter(tag.correct == T)

for(l in LOCUS){

tag.param.int <- tag.param %>% dplyr::filter(Locus == l) %>% pull(tag.threshold)
  
 tests.tagjump.int <- readr::read_csv(file.path(here::here(), "02_Results/04_ESVtable_correction", paste0("00_tagjump.threshold.control_",RUN, "_",l, ".csv")))
 tests.tagjump.int$LOCUS <- l
  tests.tagjump.int <- tests.tagjump.int %>% dplyr::filter(LOCUS == l, 
                                      project %in% c(projet, "ALL", "all", "All")) 
 

   tag.gg.1 <- tests.tagjump.int %>% 

    ggplot(aes(x = sample, y = factor(threshold), fill = abundance + 1)) +
    geom_bin2d(color = "darkgray")+
    scale_fill_distiller(trans = "log10",
                         palette = "Spectral",
                         na.value = "white",
                         name = "N reads +1") +
    theme_minimal()+
    facet_grid(. ~ Type_echantillon, scale = "free", space = "free")+
    labs(x="", y="Threshold") + 
     theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        #strip.text.y = element_text(angle = 0),
        #strip.text.x = element_text(angle = 90),
        panel.spacing = unit(0, "in"),
        legend.title = element_text(angle = 0, vjust = 0.9))

  tag.gg.2 <- tests.tagjump.int %>% 
    ggplot(aes(x = sample, y = factor(threshold), fill = richness + 1)) +
    geom_bin2d(color = "darkgray")+
    scale_fill_distiller(trans = "log10",
                         palette = "Spectral",
                         na.value = "white",
                         name = "N MOTUs +1") +
    theme_minimal()+
    facet_grid(. ~ Type_echantillon, scale = "free", space = "free")+
    labs(x="", y="Threshold") + 
    theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        #strip.text.y = element_text(angle = 0),
        #strip.text.x = element_text(angle = 90),
        panel.spacing = unit(0, "in"),
        legend.title = element_text(angle = 0, vjust = 0.9)) 
  
  
  tag.gg.3 <- ggpubr::ggarrange(tag.gg.1 + ggtitle( paste("Comparaison des seuils de tag jumping au", l, "(seuil =", tag.param.int, ")")),
                                tag.gg.2,
                                nrow = 2
                                )
 
 print(tag.gg.3)
 
 }


```

Des figures diagnostiques supplémentaires sur les seuils testés dans ce projet se trouve à l'[Annexe II](#annexe-ii).

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(nrow(tag.param) == length(LOCUS)){
  
  tag.text <- "une correction pour le tag jumping a été appliquée à tous les locus"
  
} 

if(nrow(tag.param) == 0){
  
  tag.text <- "aucune correction pour le tag jumping a été appliquée au jeu de données"
  
}

if(between(nrow(tag.param), 1, length(LOCUS) -1 )){
  
  tag.text <- paste("une correction pour le tag jumping a été appliquée aux locus suivants :", paste(tag.param$Locus, collapse = ", "))
  
}  

```

**Dans ce projet, `r tag.text`.**

\newpage

## 5.2 Classification des échantillons d'après la profondeur de séquençage et la contamination

Le tag jumping et les contaminants ont un effet plus marqué pour certains échantillons. Certains échantillons peuvent présenter très peu de reads, par exemple, parce qu'il y a eu de l'inhibition de la réaction PCR, ou que le locus utilisé est trop spécifique.    

```{r echo=FALSE, message=FALSE, warning=FALSE}
depth.param <- metabar.param %>% dplyr::filter(pcr.correct == T)

if(nrow(depth.param) == length(LOCUS)){
  
  depth.text <- "les échantillons identifiés comme problématiques ont été retirés à tous les locus"
  
} 

if(nrow(depth.param) == 0){
  
  depth.text <- "aucune correction pour les échantillons problématiques n'a été appliquée au jeu de données"
  
}

if(between(nrow(depth.param), 1, length(LOCUS) -1 )){
  
  depth.text <- paste("les échantillons identifiés comme problématiques ont été retirés aux locus suivants :", paste(depth.param$Locus, collapse = ", "))
  
}  


nreads.res %>% filter(Step %in% c("ESVdada2", "ESVtagjump", "ESVfinal"),
                      Run == RUN,
                      ID_labo %in% SUBGROUP.ls[[projet]]) %>% 
  left_join(metabar.param, by = c("Loci" = "Locus")) %>% 
  ggplot(aes(x = Nreads1, fill = Type_echantillon)) +
  geom_histogram() +
  geom_vline(aes(xintercept = depth.threshold), lty=2, color="orange") +
  scale_x_continuous(trans="log10") +
  facet_grid(Step ~ Loci, scales = "free_x") +
    labs(x = "N reads + 1 (log)", title = "Profondeur de séquençage")+ 
  theme_bw()+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

```

Un seuil minimum de nombre de reads est déterminé par locus (ligne pointillée orangée). Les échantillons sous ce seuil sont identifiés, et leur nombre peut varier entre locus. Il est important de s'interroger sur les causes possibles d'échantillons présentants un faible nombre de reads (e.g., inhibition, locus très spécifique).


```{r echo=FALSE, message=FALSE, warning=FALSE}
low.depth.sample <- nreads.res %>% filter(Step %in% c("ESVtagjump"),
                      ID_labo %in% SUBGROUP.ls[[projet]],
                      Run == RUN,
                      Type_echantillon %in% c("ECH")) %>% 
  left_join(metabar.param, by = c("Loci" = "Locus")) %>% 
  group_by(Loci) %>% 
  mutate(SUM = n()) %>% 
  filter(Nreads < depth.threshold)

data.frame(Loci = LOCUS) %>% left_join(low.depth.sample %>% group_by(Loci, SUM) %>% summarise(N = n()) %>% 
                                         mutate(Proportion = N/SUM)) %>% 
   mutate(N = ifelse(is.na(N), 0, N),
          Proportion = ifelse(N == 0, 0, round(Proportion, 2))
          ) %>%  arrange(Loci) %>% select(-SUM) %>% knitr::kable(caption = "Échantillons sous le seuil minimal de reads")
  
```

Un seuil de contamination maximale est déterminé afin d'identifier les échantillons plus problématiques dans l'interprétation. Voir la section suivante plus de détails sur l'identification des contaminants.


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=7}

summary.artefact.pcr <- data.frame(stringsAsFactors = F)

for(l in LOCUS){

  summary.artefact.pcr.int <- readr::read_csv(file.path(here::here(), "02_Results/04_ESVtable_correction", paste0("05_Artefact_PCRs_",l, "_", projet,".csv")))

summary.artefact.pcr.int <- summary.artefact.pcr.int %>% mutate(Loci = l)  
  
summary.artefact.pcr <- bind_rows(summary.artefact.pcr, summary.artefact.pcr.int)
  
}

graph.artefact.pcr<-  summary.artefact.pcr  %>% 
  filter(dataset == "tagjump") %>% 
  ggplot(aes(x=1, y = prop, fill=artefact_type)) +
  geom_bar(stat = "identity") +  #xlim(0, 1) +
  labs(fill="Category") + 
  coord_polar(theta="y") + theme_void() + 
  scale_fill_manual(limits = c("Not artefactual", "Low sequencing depth", "Contamination > 10%", "Contamination > 10% and low sequencing depth"), values = c("deepskyblue1", "darkgoldenrod1", "darkorange1", "darkred" ), labels =  c("Acceptable depth", "Low depth", "High contamination", "High contamination and low depth")) + 
  geom_text(aes(y = 0, label = paste0("n=",SUM)), vjust = 1, col = "black", cex = 5) +
  facet_wrap( ~ Loci) +
  ggtitle("Classification des différents échantillons")
graph.artefact.pcr

```

**Dans ce projet, `r depth.text`.**

\newpage

## 5.3 Identification des MOTUs contaminant

Les contaminants sont analysés en comparant les échantillons biologiques aux contrôles négatifs. Un MOTUs est identifié comme contaminant lorsque la fréquence maximale observée dans un témoin négatif est supérieure à celle observée dans les échantillons.

```{r echo=FALSE, message=FALSE, warning=FALSE}

conta.summary <- data.frame()
taxa.summary <- data.frame()
all.summary <- data.frame()

for(l in LOCUS){
  
  conta.int <- readr::read_csv(file.path(here::here(), "02_Results/04_ESVtable_correction", paste0("03_conta_",l,"_",projet, ".csv"))) 
  conta.int <- conta.int %>% mutate(Loci = l)
  conta.summary <- bind_rows(conta.summary, conta.int)

  taxa.int <- readr::read_csv(file.path(here::here(), "02_Results/04_ESVtable_correction", paste0("03_exclude.taxa_",l,"_",projet, ".csv"))) 
  taxa.int <- taxa.int %>% mutate(Loci = l)
  taxa.summary <- bind_rows(taxa.summary, taxa.int)
   
  
  all.int <- readr::read_csv(file.path(here::here(), "02_Results/04_ESVtable_correction", paste0("03_detected.controls_",l,"_",projet, ".csv"))) 
  all.int <- all.int %>% mutate(Loci = l)
  all.summary <- bind_rows(all.summary, all.int)
   
}

# Rouler seulement s'il y a des contaminants
if(nrow(conta.summary) >= 0){

conta.summary %>% mutate(Taxon = ifelse(is.na(Taxon), "Unassigned", Taxon)) %>% 
  filter(!is.na(sub.tagclean.max)) %>% 
  group_by(Loci) %>% 
  summarise(`N motus` = n(),
            `N reads` = sum(sub.tagclean.max)#,
            #Taxon = paste(unique(Taxon), collapse = ", ")
            ) %>% 
   knitr::kable(caption = "Information sur les contaminants identifiés")
}

# Rouler seulement s'il y a des contaminants
if(nrow(taxa.summary) >= 0){

taxa.summary %>%
  filter(!is.na(Tagjump.corrected)) %>% 
  group_by(Loci,Contaminant) %>% 
  summarise(`N motus` = n(),
            `N reads` = sum(Tagjump.corrected)) %>% 
   knitr::kable(caption = "Information sur les taxons sur la liste d'exclusion détectées dans les échantillons")
}

# Rouler seulement s'il y a des contaminants
if(nrow(all.summary) >= 0){

all.summary %>% mutate(Taxon = ifelse(is.na(Taxon), "Unassigned", Taxon)) %>% 
  filter(!is.na(Tagjump.corrected)) %>% 
  group_by(Loci,Contaminant, Exclude.taxa) %>% 
  summarise(`N motus` = n(),
            `N reads` = sum(Tagjump.corrected)#,
            #Taxon = paste(unique(Taxon), collapse = ", ")
) %>% 
   knitr::kable(caption = "Information sur l'ensemble des MOTUs observés dans les controles négatifs")

}


conta.param <- metabar.param %>% dplyr::filter(motus.correct == T)
taxa.param <- metabar.param %>% dplyr::filter(taxa.correct == T)

control.param <- metabar.param %>% dplyr::filter(motus.control.remove == T)


  conta.text <- paste("les MOTUs identifiés comme contaminants ont été retirés aux locus suivants :", paste(conta.param$Locus, collapse = ", "), ", et ceux sur la liste d'exclusion des taxons ont été retirés aux locus suivants :", paste(taxa.param$Locus, collapse = ", ")  )

if(nrow(conta.param) == length(LOCUS) & nrow(taxa.param) == length(LOCUS)){
  conta.text <- "les MOTUs identifiés comme contaminants et/ou sur la liste d'exclusion des taxons ont été retirés à tous les locus"
} 

if(nrow(conta.param) == 0 & nrow(taxa.param) == 0){
  conta.text <- "aucune correction pour les contaminants identifiés et la liste d'exclusion des taxons n'a été appliquée au jeu de données"
}

if(nrow(conta.param) == length(LOCUS) & nrow(taxa.param) == 0){
  conta.text <- "aucune correction pour les contaminants identifiés n'a été appliquée au jeu de données mais les MOTUs identifiés sur la liste d'exclusion des taxons ont été retirés à tous les locus"
}

if(nrow(conta.param) == 0 & nrow(taxa.param) == length(LOCUS)){
  conta.text <- "aucune correction pour la liste d'exclusion des taxons n'a été appliquée au jeu de données mais les MOTUs identifiés comme contaminants ont été retirés à tous les locus"
}

  
conta.text2 <- paste("Les MOTUs observés dans les contrôles négatifs été retirés aux locus suivants :", paste(control.param$Locus, collapse = ", ")  )

if(nrow(control.param) == length(LOCUS)){
  conta.text2 <- "Les MOTUs observés dans les contrôles négatifs ont été retirés à tous les locus"
} 

if(nrow(control.param) == 0){
  conta.text2 <- "Les MOTUs observés dans les contrôles négatifs n'ont pas été systématiquement exclus du jeu de données"
}


```
Chaque MOTU est classé dans une catégorie.

```{r echo=FALSE, message=FALSE, warning=FALSE}

summary.artefact.motus <- data.frame(stringsAsFactors = F)

for(l in LOCUS){

  summary.artefact.motus.int <- readr::read_csv(file.path(here::here(), "02_Results/04_ESVtable_correction", paste0("05_Artefact_MOTUs_",l, "_", projet,".csv")))

summary.artefact.motus.int <- summary.artefact.motus.int %>% mutate(Loci = l)  
  
summary.artefact.motus <- bind_rows(summary.artefact.motus, summary.artefact.motus.int)
  
}

graph.artefact.motus <-  summary.artefact.motus  %>% 
  filter(dataset == "tagjump") %>% 
  ggplot(aes(x=1, y = prop, fill=artefact_type)) +
  geom_bar(stat = "identity") +  #xlim(0, 1) +
  labs(fill="Category") + 
  coord_polar(theta="y") + theme_void() + 
      scale_fill_manual(limits = c("Good MOTU", "Contaminant - excluded taxa", "Contaminant - included taxa", "Excluded taxa", "Detected in controls only"), values = c("deepskyblue1", "brown","red" , "orange", "pink" )) + 
  geom_text(aes(y = 0, label = paste0("n=",SUM)), vjust = 1, col = "black", cex = 3) +
  facet_grid(Loci ~ level) +
  ggtitle("Classification des différents MOTUs")
graph.artefact.motus

```

Au delà des reads considérés comme contaminants, il peut rester certaines détections dans les contrôles négatifs. Les MOTUs et leurs taxons associés détectés dans les échantillons contrôles sont les suivants: 

```{r echo=FALSE, message=FALSE, warning=FALSE,  fig.height=5, fig.width=9,}
ESV.table.control.long <- data.frame(stringsAsFactors = F)

for(l in LOCUS){

MOTUs.control.table <- readr::read_csv(file.path(here::here(), "00_Data/04_ESVcorrected", paste0("MOTUs.Metabarinfo.postTagjump_",RUN,"_", l, "_ALL.csv")))
ESV.control.table   <- readr::read_csv(file.path(here::here(), "00_Data/04_ESVcorrected", paste0("ESVtab.postTagjump_",RUN,"_", l, "_ALL.csv")))
names(ESV.control.table)[1] <- "ID_labo" 

ESV.table.control.long.int <- ESV.control.table %>% pivot_longer(-ID_labo, names_to = "ID", values_to = "Nreads") %>% 
                  mutate(Loci = l) %>% 
                  left_join(data.info %>% select(ID_labo, ID_projet, Type_echantillon, Station, Site_echantillonnage) %>%  distinct(.keep_all = T )) %>% 
  filter(ID_labo %in% SUBGROUP.ls[[projet]],
         Type_echantillon %nin% c("ECH", "PPC", "MPC")) %>% 
        left_join(MOTUs.control.table %>% select(-Loci), by = c("ID" = "ESV"))
ESV.table.control.long <- bind_rows(ESV.table.control.long, ESV.table.control.long.int)
   
}


for(l in LOCUS){

resi.int <-   ESV.table.control.long %>% 
    mutate(Taxon= ifelse(is.na(Taxon), "Unassigned", Taxon),
           Category = ifelse(not_a_max_conta == FALSE & not_an_exclude_taxa == TRUE, "Contaminant",
                      ifelse(not_a_max_conta == FALSE & not_an_exclude_taxa == FALSE, "Contaminant + exclusion taxa list",
                      ifelse(not_a_max_conta == TRUE & not_an_exclude_taxa == FALSE, "Exclusion taxa list only",       
                      ifelse(not_a_max_conta == TRUE & not_an_exclude_taxa== TRUE, "Good MOTU", "Undefined??"))))
    
           ) %>% 
  filter(#Nreads > 0,
         Loci == l) %>% 
  group_by(Category, Loci, ID_labo, Type_echantillon, Taxon) %>% 
  summarise(Nreads = sum(Nreads)) %>% 
  mutate(Taxon_loci_Cat = paste(Taxon, Loci, Category))
  
focus.MOTUs <- resi.int %>% filter(Nreads > 0) %>% pull(Taxon_loci_Cat) %>% unique()

if(length(focus.MOTUs) > 0){

graph.resi <- resi.int   %>% filter(Taxon_loci_Cat %in% focus.MOTUs) %>% 
  ggplot(aes(fill = Nreads , x = ID_labo, y = Taxon)) +
  labs(x= "", y = "") + 
  geom_bin2d(color = "darkgray")+
  scale_fill_distiller(trans = "log10",
                       palette = "Spectral",
                       na.value = "white"#,
                       #breaks = c(1, 10, 100, 1000, 10000, 100000, 1000000,10000000), labels = c("1", "10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")
                       ) +
  theme_minimal()+
  facet_grid(Category ~ Type_echantillon, scale = "free", space = "free") + 
  ggtitle(paste("Détections dans les contrôles négatifs au locus", l)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        strip.text.y = element_text(angle = 0),
        strip.text.x = element_text(angle = 90),
        panel.spacing = unit(0, "in"),
        legend.text = element_text(angle = 45, hjust = 1),
        legend.title = element_text(angle = 0, vjust = 0.9),
        legend.position = "bottom")

print(graph.resi)
}

}

```

**Dans ce projet, `r conta.text`. `r conta.text2`.**

\newpage

## 5.4 Autres aspects à considérer

D'autres corrections pourraient être appliquées au jeu de données: 

- Les MOTUs pourraient être filtrés sur la longueur de leur séquence.
- Un minimum de reads par MOTUs par échantillon pourrait être ajouté selon les besoins d'analyses subséquents.
- S'il y avait des réplicats biologiques, ils pourraient également être utilisés pour identifier d'autres problématiques.

\newpage

# 6. Évaluation sommaire du jeu de données final 

## 6.1 Diversité observée

La relation entre la diversité (N MOTUs) et la profondeur de séquençage (N reads) permet d'évaluer si la profondeur de séquençage était suffisante. Un plateau devrait être observé lorsque la profondeur de séquençage nécessaire est atteinte. 

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 3, fig.width=7}

# for(l in LOCUS){
#   
#   esv.path <- file.path(here::here(), "00_Data", "03c_ESV", paste0("ESV.", l, "_table.txt"))
#   
#   seq.int <- readr::read_delim(esv.path, delim = " ", col_names = F, n_max = 1) %>% as.vector()
#   esv.int <- readr::read_delim(esv.path, delim = " ", col_names = F, skip = 1) 
#   names(esv.int) <- c("ID_labo", seq.int)
#   esv.int <- esv.int %>% pivot_longer(-ID_labo, names_to = "SEQ", values_to = "Nreads") %>% 
#                          mutate(Loci = l)
#   
#   ESV.table <- bind_rows(ESV.table, esv.int)
# 
# }
#  
##str(ESV.table)#
#

#ESV.table <- ESV.table %>% left_join(data.info %>% select(ID_labo, ID_projet, Loci, Type_echantillon))

#ESV.table.summary <- ESV.table %>% group_by(ID_labo, Loci, Type_echantillon) %>% 
#              summarise(NESV = length(SEQ[Nreads > 0]),
#                        Nreads = sum(Nreads)) 

metabar.summary %>%  
  ggplot(aes(y = nb_motus_postmetabaR, x = nb_reads_postmetabaR)) +
                    geom_point(alpha = 0.5) +
    #scale_y_continuous(trans="log10") +
    #scale_x_continuous(trans="log10") +
  facet_grid(. ~ Loci, scale = "free_x") +
  #geom_smooth(method = "lm") +
   labs(y = "N motus ", x = "N reads", title = "Relation entre la diversité de MOTUs et la profondeur de séquençage")+ 
  theme_bw()+
  theme(#legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust =1, vjust = 1))


```

En pratique, les erreurs de PCRs et les approches ou paramètres utilisés pour obtenir les MOTUs peuvent créer des erreurs et certains MOTUs peuvent représenter des artéfacts d'analyse. Pour tenter de pallier ce problème, il est possible de regarder plutôt la relation avec le nombre de taxons uniques détectés - même si une grande proportion n'a pas d'assignation taxonomique.  

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height= 3, fig.width=7}
ESV.table.long <- data.frame(stringsAsFactors = F)

for(l in LOCUS){

MOTUs.table <- readr::read_csv(file.path(here::here(), "00_Data/04_ESVcorrected", paste0("MOTUs.Metabarinfo.corrected_",l, "_",projet, ".csv")))
ESV.table   <- readr::read_csv(file.path(here::here(), "00_Data/04_ESVcorrected", paste0("ESVtab.corrected_",l, "_",projet, ".csv")))
names(ESV.table)[1] <- "ID_labo" 

ESV.table.long.int <- ESV.table %>% pivot_longer(-ID_labo, names_to = "ID", values_to = "Nreads") %>% 
                  mutate(Loci = l) %>% 
                  left_join(data.info %>% select(ID_labo, ID_projet, Type_echantillon, Station, Site_echantillonnage) %>%  distinct(.keep_all = T )) %>% 
                  left_join(MOTUs.table %>% select(-Loci), by = c("ID" = "ESV"))
ESV.table.long <- bind_rows(ESV.table.long, ESV.table.long.int)
   
}


ESV.table.long %>% 
  mutate(Taxon = ifelse(is.na(Taxon), "Unassigned", Taxon)) %>% 
  group_by(Loci, ID_labo) %>% 
  summarise(nb_taxon_postmetabaR = length(unique(Taxon[Nreads>0])),
            nb_reads_postmetabaR = sum(Nreads)) %>% 
  ggplot(aes(y = nb_taxon_postmetabaR, x = nb_reads_postmetabaR)) +
                    geom_point(alpha = 0.5) +
    #scale_y_continuous(trans="log10") +
    #scale_x_continuous(trans="log10") +
  facet_grid(. ~ Loci, scale = "free_x") +
  #geom_smooth(method = "lm") +
   labs(y = "N Taxons", x = "N reads", title = "Relation entre la diversité de taxons et la profondeur de séquençage")+ 
  theme_bw()+
  theme(#legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust =001, vjust = 1))

```

La diversité peut aussi être évaluée à l'aide de différentes métriques comme le N de MOTUs, de taxons ou de reads assignés à de hauts niveaux phylogénétiques.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=6}


# ESV.table.long %>% 
#   mutate(group = ifelse(is.na(Taxon), "Unassigned", 
#                         ifelse(Taxon == "Homo sapiens", "Human",
#                         #is.na(phylum), "Unassigned", 
#                         ifelse(phylum %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera"), phylum, "Others"))),
#          group = factor(group, levels =  c("Arthropoda",  "Echinodermata", "Porifera", "Cnidaria", "Chordata",  "Human", "Others",  "Unassigned"))) %>% 
#   group_by(Loci, group) %>% 
#   summarise(motus = length(unique(ID)),
#             reads = sum(Nreads),
#             taxons = length(unique(Taxon))) %>% 
#   pivot_longer(cols = c(motus, reads, taxons), names_to = "level", values_to = "N") %>% 
#   ungroup() %>% 
#   group_by(Loci, level) %>% 
#   mutate(SUM = sum(N),
#          prop = N/SUM,
#          level = factor(level, levels = c("motus", "taxons", "reads"))) %>% 
#   ggplot(aes(x=level, y = prop, fill = forcats::fct_rev(group))) +
#   geom_bar(stat = "identity") +  #xlim(0, 1) +
#   scale_fill_brewer(palette = "Set2", direction=-1) +
#   labs(fill="Groupe") + 
#   #coord_polar(theta="y") + theme_void() + 
#   #scale_fill_manual(limits = c("Not artefactual", "Contamination"), values = c("deepskyblue1", "darkorange1" )) + 
#   geom_text(aes(y = 0.75, x= 1, label = paste0("n=",SUM)), vjust = 1, hjust = 0, col = "black", cex = 3) +
#   facet_grid(Loci ~ level) +
#    ggforce::facet_zoom(y = group %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera"), zoom.size = 1) +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
#   ggtitle("Classification à différents groupes taxonomiques")
# 

for(l in LOCUS){

data.int <- ESV.table.long %>% 
  mutate(group = ifelse(is.na(Taxon), "Unassigned", 
                        ifelse(Taxon == "Homo sapiens", "Human",
                        #is.na(phylum), "Unassigned", 
                        ifelse(phylum %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera"), phylum, "Others"))),
         group = factor(group, levels =  c("Arthropoda",  "Echinodermata", "Porifera", "Cnidaria", "Chordata",  "Human", "Others",  "Unassigned"))) %>% 
  group_by(Loci, group) %>% 
  summarise(motus = length(unique(ID)),
            reads = sum(Nreads),
            taxons = length(unique(Taxon))) %>% 
  pivot_longer(cols = c(motus, reads, taxons), names_to = "level", values_to = "N") %>% 
  ungroup() %>% 
  group_by(Loci, level) %>% 
  mutate(SUM = sum(N),
         prop = N/SUM,
         level = factor(level, levels = c("motus", "taxons", "reads")),
         SUM.text = ifelse(SUM > 1000000, paste(round(SUM/1000000,1), "M"), as.character(SUM))) %>% 
  filter(Loci == l) 

min.class <- data.int %>% filter(group %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera")) %>% 
             pull(prop) %>% min()

sum.class <- data.int %>% filter(group %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera")) %>% 
            group_by(level) %>% summarise(sum = sum(prop))  %>% pull(sum) %>% max()

graph.int <-  data.int %>% 
  ggplot(aes(x=level, y = prop, fill = forcats::fct_rev(group))) +
  geom_bar(stat = "identity") +  #xlim(0, 1) +
  scale_fill_brewer(palette = "Set2", direction=1, limits = c("Arthropoda",  "Echinodermata", "Porifera", "Cnidaria", "Chordata",  "Human", "Others",  "Unassigned") ) +
  #scale_y_continuous(limits = c(0,1.1), breaks = c(0,0.2,.4,.6,.8,1))+
  labs(fill="Groupe") + 
  #coord_polar(theta="y") + theme_void() + 
  #scale_fill_manual(limits = c("Not artefactual", "Contamination"), values = c("deepskyblue1", "darkorange1" )) + 
  geom_text(aes(y = 0.95, x= level, label = paste0("n=",SUM.text)), vjust = 0.5, hjust = 0.5, col = "black", cex = 3) +
  #facet_grid(Loci ~ level) +
   
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  ggtitle(paste("Classification à différents groupes taxonomiques pour", l))

if(min.class < .10 & sum.class > .5){
 graph.int <- graph.int  + ggforce::facet_zoom(y = group %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera"), zoom.size = 1, ylim = c(0, 0.5)) 
}

if(min.class < .10 & sum.class < .5){
 graph.int <- graph.int  + ggforce::facet_zoom(y = group %in% c("Arthropoda", "Chordata", "Cnidaria", "Echinodermata", "Porifera"), zoom.size = 1, ylim = c(0, sum.class)) 
}



print(graph.int)

}



```

\newpage

## 6.2 Diversité par échantillon (PCR)

Un coup d'oeil rapide sur les résultats par échantillon à tous les locus. **À noter que les assignations taxonomiques n’ont pas été révisées par un expert.**

```{r echo=FALSE, fig.height=9, fig.width=9, message=FALSE, warning=FALSE}
n.stations <- ESV.table.long %>% pull(Station) %>% unique() %>% length()
n.sites <- ESV.table.long %>% pull(Site_echantillonnage) %>% unique() %>% length()

for(l in LOCUS){
graph.int <-   ESV.table.long %>% 
    mutate(Taxon= ifelse(is.na(Taxon), "Unassigned", Taxon),
         Groupe = ifelse(Loci %in% c("MiFishU", "MiMam", "16Schord"), class, phylum)#,
         #Nreads = ifelse(Nreads == 1, 0 , Nreads)
         ) %>% 
  filter(Loci == l,
         phylum %in% c("Arthropoda", "Cnidaria", "Echinodermata", "Chordata", "Mollusca", "Porifera", "Nemertea", "Bryozoa", "Annelida","Brachiopoda", "Sipuncula", "Chlorophyta")) %>%
  group_by(Loci, ID_labo, Station, Site_echantillonnage, Taxon, Groupe) %>% 
  summarise(Nreads = sum(Nreads)) %>% 
  ggplot(aes(fill = Nreads, x = ID_labo, y = Taxon)) +
  labs(x= "", y = "") + 
  geom_bin2d(color = "darkgray")+
  scale_fill_distiller(trans = "log10",
                       palette = "Spectral",
                       na.value = "white"#,
                       #breaks = c(1, 10, 100, 1000, 10000, 100000, 1000000,10000000), labels = c("1", "10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")
                       ) +
  theme_minimal()+
  facet_grid(Groupe ~ ., scale = "free", space = "free") + 
  ggtitle(paste("Détection par échantillon au locus", l)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        strip.text.y = element_text(angle = 0),
        strip.text.x = element_text(angle = 90),
        panel.spacing = unit(0, "in"),
        legend.text = element_text(angle = 45, hjust = 1),
        legend.title = element_text(angle = 0, vjust = 0.9),
        legend.position = "bottom")

if(n.stations > 1 & n.sites == 1){
  graph.int <- graph.int  + facet_grid(Groupe ~ Station, scale = "free", space = "free")    
}

if(n.sites > 1 & n.stations == 1){
  graph.int <- graph.int  + facet_grid(Groupe ~ Site_echantillonnage, scale = "free", space = "free")    
}

if(n.sites > 1 & n.stations > 1){
  graph.int <- graph.int  + facet_grid(Groupe ~ Station + Site_echantillonnage, scale = "free", space = "free")    
}


print(graph.int)

}

```

```{r include=FALSE}
# En profiter pour extraire les données

readr::write_csv(ESV.table.long %>% group_by(Loci, ID_labo, Station, Site_echantillonnage, Taxon, species, genus, family, order, class, phylum) %>% summarise(Nreads = sum(Nreads)), 
                 file = paste0(projet, "_ESVtab_taxo_report.csv"))

```



\newpage


# Annexe I
## Détails sur le pipeline d'analyse

Les scripts et résultats se trouvent sur GitHub dans le répertoire **[`r git.url %>% str_remove("https://github.com/")`](`r git.url`)**.

### 1. Obtention d'une table d'ESV

Script associé : **02_Process_RAW.R**

Les étapes sont :

1. `cutadapt` pour enlever les adapteurs

2. `dada2::filterAndTrim` avec les paramètres suivants :

```{r echo=FALSE, message=FALSE, warning=FALSE}
PARAM.DADA2 <- readr::read_tsv(file.path(here::here(), "01_Code/Parameters/dada2_param.tsv"))
PARAM.DADA2 %>% dplyr::filter(Locus %in% LOCUS) %>%   knitr::kable()

```
3. `dada2::learnErrors` pour apprendre le taux d'erreur, par locus

```
nbases = 1e8
randomize = T
MAX_CONSIST = 10
```

4. `dada2::derepFastq` pour dérepliquer les échantillons

5. `dada2::dada` pour inférer chaque échantillons

6. `dada2::mergePairs` pour joindre les R1 et R2

```
minOverlap = 30 
maxMismatch = 0
```

7. `dada2::removeBimeraDenovo`

```
method = "consensus"
```

Version des ressources utilisées:

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(file.exists(file.path(here::here(), "00_Data/03c_ESV/Process_RAW.log"))){

res <- readr::read_lines(file = file.path(here::here(), "00_Data/03c_ESV/Process_RAW.log"),
                         skip = 7
                         )


cat(res, sep = "\n")
}
```

### 2. Assignations taxonomiques

Scripts associés : **03a_TaxoAssign_Blast.R** pour préparer le jeu de données, **03b_TaxoAssign_Blast.R** pour les assignations blast avec NCBI-nt, **03b_TaxoAssign_Blast_local.R** pour les assignations blast avec une banque de référence locale, **03c_TaxoAssign_RDP.R** pour les assignations avec RDP, et **03c_TaxoAssign_CompareResults.R** pour créer le fichier d'assignation taxonomique final.

Les assignations taxonomiques ont été faites avec les paramètres suivants:

```{r echo=FALSE, message=FALSE, warning=FALSE}

blast.param <- readr::read_tsv(file = file.path(here::here(), "01_Code/Parameters/blast_param.tsv"))

blast.param <- blast.param  %>% mutate(evalue = as.character(evalue))
blast.param %>% dplyr::filter(Locus %in% LOCUS) %>% 
               dplyr::select(-db) %>%  knitr::kable(caption ="Paramètres généraux de Blast")


rdp.param <- readr::read_tsv(file = file.path(here::here(), "01_Code/Parameters/rdp_param.tsv"))

rdp.param %>% dplyr::filter(Locus %in% LOCUS) %>% 
               knitr::kable(caption ="Paramètres généraux de RDP")


```

Version des ressources utilisées:

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(file.exists(file.path(here::here(), "02_Results/03_TaxoAssign/01_Blast_nt", "TaxoAssign_Blast.log"))){

res <- readr::read_lines(file = file.path(here::here(), "02_Results/03_TaxoAssign/01_Blast_nt", "TaxoAssign_Blast.log"),
                         skip = 7
                         )


cat("\nBlast avec NCBI-nt (Genbank)",res, sep = "\n")
}

if(file.exists(file.path(here::here(), "02_Results/03_TaxoAssign/03_Blast_local", "TaxoAssign_Blast.log"))){

res <- readr::read_lines(file = file.path(here::here(), "02_Results/03_TaxoAssign/03_Blast_local", "TaxoAssign_Blast.log"),
                         skip = 7
                         )


cat("\nBlast avec une banque de référence locale",res, sep = "\n")
}


if(file.exists(file.path(here::here(), "02_Results/03_TaxoAssign/02_RDP", "TaxoAssign_RDP.log"))){

res <- readr::read_lines(file = file.path(here::here(), "02_Results/03_TaxoAssign/02_RDP", "TaxoAssign_RDP.log"),
                         skip = 0
                         )


cat("\nRDP",res, sep = "\n")
}


```

### 3. Correction des tables d'ESV

Script associé : **04_ESVtable_correction.R**

Les corrections ont été faites à l'aide du paquet `metabaR`. Les étapes sont :

1. Identifier le tagjumping sur l'ensemble des échantillons d'une même run à l'aide de la fonction `tagjumpslayer` (threshold = *tag.threshold*) et les enlever (__facultatif__, tag.correct = *TRUE*). 

2. Identifier les échantillons (pcr) sur l'ensemble des échantillons d'une même run avec peu de profondeur de séquençage (threshold = *depth.threshold*) et les enlever (__facultatif__, pcr.correct = *TRUE*).  

3. Identifier les contaminants spécifiques à un projet à l'aide de la fonction `contaslayer` (method = max, control_types = c("pcr", "extraction")) et les enlever (__facultatif__, motus.correct = *TRUE*). 

4. Identifier les assignations de taxons sur la liste d'exlusion (Table 10) et les enlever (__facultatif__, taxa.correct = *TRUE*)

5. Identifier les échantillons spécifiques à un projet présentants une forte proportion de contaminants (threshold = *prop.cont.threshold*) et les enlever (__facultatif__, pcr.correct = *TRUE*). 

Les paramètres utilisés sont les suivants :

```{r echo=FALSE, message=FALSE, warning=FALSE}

metabar.param <- readr::read_tsv(file = file.path(here::here(), "01_Code/Parameters/metabar_param.tsv"))

# Split into 3 tables (too big)
metabar.param %>% dplyr::filter(Locus %in% LOCUS) %>% dplyr::select(Locus, tag.threshold, tag.correct) %>% knitr::kable(caption = "Paramètres pour le tag jumping", col.names = str_replace_all(names(.), "[.]", " " ))

metabar.param %>% dplyr::filter(Locus %in% LOCUS) %>% dplyr::select(Locus, motus.correct, taxa.correct,	motus.control.subtract,	motus.control.remove) %>% knitr::kable(caption = "Paramètres pour les MOTUs", col.names = str_replace_all(names(.), "[.]", " " ))

metabar.param %>% dplyr::filter(Locus %in% LOCUS) %>% dplyr::select(Locus,	depth.threshold,	prop.cont.threshold, pcr.correct) %>% knitr::kable(caption = "Paramètres pour les échantillons", col.names = str_replace_all(names(.), "[.]", " " ))


taxa.param <- readr::read_tsv(file = file.path(here::here(), "01_Code/Parameters/metabar_exclude_taxa.tsv"))

taxa.param %>% mutate(Level = factor(Level, levels = c("kingdom", "phylum", "order", "class", "family", "genus", "species")) ) %>%  arrange(Level, ID) %>%  knitr::kable(caption = "Taxons présents sur la liste d'exclusion", col.names = str_replace_all(names(.), "[.]", " " ))

```

Version des ressources utilisées:

```{r echo=FALSE, message=FALSE, warning=FALSE}
if(file.exists("00_Data/04_ESVcorrected/ESVtab_correction.log")){

res <- readr::read_lines(file = file.path(here::here(), "00_Data/04_ESVcorrected", paste0("ESVtab_correction_",RUN,"_.log")),
                         skip = 7
                         )


cat(res, sep = "\n")
}
```

\newpage

# Annexe II

## Correction pour le tag jumping

Le choix de ce seuil (ligne orange pointillée) est important car il devrait réfléter un point d'inflexion où il y a une diminution dans la diversité des MOTUs sans observer une diminution du nombre de reads (abondance). Un changement dans le seuil peut influencer de manière importante les résultats en termes de faux-négatifs et faux-positifs.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=4}
load( file.path(here::here(), "02_Results/04_ESVtable_correction", paste0( "00_tag.gg.",RUN,".Rdata")))

for(l in LOCUS){
print(get( paste0("tag.gg.", l))) 

}

```

Il est possible de vérifier, pour le MOTUs le plus abondant, l'impact du seuil choisi.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.height=10, out.extra="page=2"}

for(l in LOCUS){

  print(get( paste0("plate.tag.gg.", l))) 

}

```
